<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/svg href=https://dm4p385.github.io/gbu-blog/ico/favicon.svg><title>Building a conversational Voice & Text AI Assistant - The good, the bad, and the [undefined]</title><meta name=description content="A blog about tech, engineering and all things in between."><meta name=keywords content="RAG,AI,LLMs,LangGraph,Whisper,Text-To-Speech,engineering,"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Inconsolata&display=swap" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://dm4p385.github.io/gbu-blog/css/main.css><link rel=stylesheet type=text/css media=screen href=https://dm4p385.github.io/gbu-blog/css/syntax.css><script src=https://unpkg.com/feather-icons></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Building a conversational Voice & Text AI Assistant"><meta name=twitter:description content="How we made a human-like voice and text AI assistant to help reduce dropoffs in our onboarding journey."><meta property="og:url" content="https://dm4p385.github.io/gbu-blog/posts/conversational-ai-assistant/"><meta property="og:site_name" content="The good, the bad, and the [undefined]"><meta property="og:title" content="Building a conversational Voice & Text AI Assistant"><meta property="og:description" content="How we made a human-like voice and text AI assistant to help reduce dropoffs in our onboarding journey."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-11T12:00:00+05:30"><meta property="article:modified_time" content="2025-11-11T12:00:00+05:30"><meta property="article:tag" content="RAG"><meta property="article:tag" content="AI"><meta property="article:tag" content="LLMs"><meta property="article:tag" content="LangGraph"><meta property="article:tag" content="Whisper"><meta property="article:tag" content="Text-to-Speech"><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("consent","default",{ad_storage:"denied",analytics_storage:"denied"})</script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-XXXXXXXXX-X")</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX-X"></script><script>for(var c,nameEQ=name+"=",ca=document.cookie.split(";"),i=0;i<ca.length;i++)c=ca[i],c.includes(nameEQ)&&gtag("consent","update",{ad_storage:"denied",analytics_storage:"granted"})</script></head><body><div class=container><header><div><h1 class=site-title><a href=https://dm4p385.github.io/gbu-blog/><span class=title-prefix>The good, the bad, and the </span><span class="bracket open">[</span><span class=undefined-inner>undefined</span><span class="bracket close">]</span></a></h1><div class=site-info><h2>A blog about tech, engineering and all things in between.</h2><nav class=social-nav><ul class=flat><li><a href=https://github.com/dm4p385 title=GitHub target=_blank><i data-feather=github></i></a></li><li><a href=https://x.com/harsh_singh58 title=X target=_blank><i data-feather=twitter></i></a></li><li><a href=https://www.linkedin.com/in/harshsingh58 title=LinkedIn target=_blank><i data-feather=linkedin></i></a></li><li><a rel=alternate type=application/rss+xml href=https://dm4p385.github.io/gbu-blog/index.xml title="The good, the bad, and the [undefined]"><i data-feather=rss></i></a></li></ul></nav></div><nav><ul class=flat><li><a href=/gbu-blog/>Home</a></li><li><a href=/gbu-blog/posts>Posts</a></li><li><a href=/gbu-blog/tags>Tags</a></li><li><a href=/gbu-blog/about>About</a></li></ul></nav></div></header><div class=content><div><h1 class=single-title>Building a conversational Voice & Text AI Assistant</h1><p class=metadata>Published - 2025-11-11 | <i data-feather=clock style=width:12px;height:12px></i> 7min</p></div><article class=markdown><h2 id=the-whys><strong>The Whys</strong></h2><p>During my time working as a SDE in the Data & AI team at Vegapay, we performed a detailed funnel analysis of one of our card onboarding journeys. During the analysis we found out several steps where the users were dropping off due to either friction or clarity. This meant that there was a clear requirement of assisting users with their onboarding much like how a real support agent could and help resolve user queries on the fly.</p><p>However, using actual human support agents is a logistics and user experience nightmare. Not all questions need to be answered by humans and the user cannot be expected to be on call all the time while he is filling out his application. With the rise of conversational AI SDKs and Agents like Eleven Labs, there existed a tech solution that could really elevate user experience and help us reduce customer dropoffs.</p><p>Eleven Labs, already offered paid SDKs and voice conversational Agents that can be integrated out of the box, however keeping in mind the costs and customer data privacy we decided to go with a in-house developed version that allowed us to keep all of our data on-prem and allowed flexibility in terms of features.</p><h2 id=the-whats><strong>The Whats</strong></h2><p>The idea was simple, develop a multi-modal (voice and chat) AI assistant that could RAG over our onboarding knowledge base and leverage this context to dynamically answer user queries. While it wasn&rsquo;t possible for me to completely replicate the standards of Eleven Lab offered solutions, We did have a clear set of objectives that this project needed to accomplish in order to be deemed a success.</p><ul><li>Respond to User queries using the onboarding journey knowledge base as a strict source of information.</li><li>Offer interactions through both speech and text.</li><li>Handle real-time user interruptions gracefully.</li><li>Be capable of understanding pauses and tones.</li><li>Maintains contextual continuity during a session.</li><li>Responds in an acceptable latency.</li></ul><h2 id=the-hows---aka-the-system-architecture><strong>The Hows - AKA The System Architecture</strong></h2><div style="margin:1rem 0;text-align:center"><img src=https://dm4p385.github.io/gbu-blog/images/p2_system_architecture.png alt="System Architecture Diagram" style=zgotmplz><figcaption style=font-size:.9rem;color:var(--muted,#666);margin-top:.4rem;text-align:center>High Level System architecture of our AI Assistant</figcaption></div><h3 id=building-a-knowledge-base><strong>Building a knowledge base</strong></h3><p>The first and foremost step of building any system like this was to formulate a strategy of building the knowledge base based on our onboarding journey. Since our onboarding process is a deterministic flow, we decided to have a pre-defined template of each and every screen where we would store:</p><ol><li>Screen/Step Name</li><li>What the user sees</li><li>What the user can do</li><li>Expected outcome</li><li>Unexpected outcome</li><li>Screen/Step FAQs</li></ol><p>This allowed me to give my LLMs context about each and every screen without having to rely on VL models or Images.</p><h3 id=chunking--retrieval-strategy---vector-dbs-vs-graph-dbs><strong>Chunking & Retrieval Strategy - Vector DBs v/s Graph DBs</strong></h3><p>In case of RAGs, how you store your data and how you retrieve your data is fundamental to how the LLMs interpret them.</p><p>My first hunch was to use graph DBs like Neo4J to create knowledge graphs out of our onboarding journey context. Knowledge graphs can help improve search relevance by accounting for the relationships between data-points, this allows us to enable multi-hop reasoning which is something that is not present in standard semantic search solutions.
However while this approach did allows us to have an enhanced contextual understanding of the knowledge base, this completely killed the responsiveness of our assistant. Querying the Graph DB meant we had to have an extra agent solely responsible for generating Graph DB queries, this meant an extra LLM call and combining this with Multi-hop strategy it meant our assistant was painfully slow.</p><p>Prioritizing responsiveness over enhanced reasoning, We elected to go with a plain old Semantic Search approach. While vector DBs do not store inherent relationships between data points, they are amazingly fast at processing and querying large datasets. This fact combined with the fact that our knowledge base didn&rsquo;t exactly require the advanced reasoning capability that knowledge graphs offered, we chunked our knowledge base in a json format based on the aformentioned template and stored them in our Vector DB.</p><h3 id=making-the-agent-listen><strong>Making the Agent listen</strong></h3><p>Having figured out the Retrieval and Augmentation parts of the RAG, it was time to focus on the Generation aspect. However before we could RAG over our knowledge base, we had to parse the user voice input into a RAG-able query.</p><p>This meant 2 things:</p><ol><li>We had to detect when the user was actually speaking, this meant applying noise compression and other required clean-ups. Recording and streaming all user audio is a very silly idea as it could easily overwhelm our system and is in general a waste of resources.</li><li>We had to parse what the user was speaking, this meant using an Audio to Text model.</li></ol><p>To solve #1, We decided to go with an approach called VAD (Voice Activity Detection) on the client side using <a href=https://github.com/snakers4/silero-vad>silero-vad</a>. This library already implements all the required required speech detection and audio clean-up/compression functionaltiy that was required for my use case. This allowed me to only stream audio when required to my underlying backend agent.</p><p>Solving #2 was even easier using OpenAI&rsquo;s whisper model. Whisper allowed me to convert my detected speech audio to text while being fast and accurate.</p><p>Having solved both of these problems, my system was still doing pretty good latency wise.</p><h3 id=making-the-agent-talk><strong>Making the Agent talk</strong></h3><p>Now that our Agent had the capability to listen and generate relevant answers based on the available context. The next step was to make it talk.
Being constrained on resources and latency We found a really good TTS model called <a href=https://huggingface.co/hexgrad/Kokoro-82M>Kokoro-TTS</a> that had support for custom pronounciation, intonation and stress levels in its speech. Being only a 82M parameter model it was blazing fast for my use case, also it supported streaming audio. This meant We would save time by streaming and playing audio chunks on the frontend as they&rsquo;re being generated on the backend. This was a huge win on the latency part!</p><h3 id=handling-interruptions><strong>Handling Interruptions</strong></h3><p>Our agent could now listen and talk with good intonation and response times! But the final frontier of making it more human-like was to add the ability to make our agent handle interruptions gracefully. In real life conversations, humans have the ability to process interruptions, drop the current conversation thread and move onto the next one. This is precisely what our Agent needed to make it more human like.</p><p>To implement the idea was simple and as follows:</p><ul><li>All the interfacing to the client would be done via WebSockets since we&rsquo;re solving a bi-directional real-time problem.</li><li>We will initialize a user session once a websocket connection is made.</li><li>Each Session&rsquo;s responsibility would be to manage the STT -> RAG -> TTS langgraph process&rsquo; lifecycle using python&rsquo;s Asyncio library. This meant each langgraph process would be one Asyncio Task.</li><li>When a user interruption was detected on the frontend, the frontend would first stop the TTS audio, clear the audio buffer and send an interrupt event to the backend. Following this, the new speech detected audio is sent to the backend.</li><li>Upon recieving the interrupt event, the Session Manager would send a Cancellation Event to the asyncio Task in process, which would stop the current LangGraph process. This now allowed the Session Manager to pick-up the new input and spawn a new asyncio Task for the same.</li></ul><p>This approach allowed us to gracefully handle user interruptions and made our Agent more human like.</p><h2 id=what-we-achieved--future-scope><strong>What we achieved + Future Scope</strong></h2><p>By the end of this initial iteration of development, we had built a fully in-house, privacy-first conversational assistant capable of operating in both voice and text modes — without relying on third-party conversational SDKs hence saving on cost while still getting the job done.</p><p>We achieved:</p><ul><li>Low-latency RAG responses using efficient semantic retrieval and streaming outputs.</li><li>Human-like voice interaction powered by OpenAI Whisper for STT and Kokoro-TTS for fast, natural speech synthesis.</li><li>Graceful interruption handling through asyncio task cancellation and session-based lifecycle management.</li><li>Grounded responses using a structured knowledge base and retrieval-first design to minimize hallucinations.</li><li>Measurable quality improvements, with ~89% of responses rated Good or Excellent across 217 test queries that we generated from our knowledge base documents.</li></ul><p>Most importantly, we proved that a responsive, interruptible, and privacy-conscious voice assistant can be built in-house without sacrificing usability.</p><p>In future iterations we developed:</p><ul><li>Chat history persistence: Not only do we store user history across sessions, we allowed the user to pick-up and continue past conversation threads.</li><li>GuardRails: By leveraging a completely parallel LLM call we were also able to compare the query and the fetched context to determine if the current query was answerable or not hence preventing haullicinations and jail-break attempts while adding zero overhead to the existing process.</li><li>User Context: By storing user onboarding events being emitted by our onboarding backend service, we were able to have a complete overview of where the user was in terms of onboarding journey, what issues he had faced (errors and such) and other metadata. This meant our Agent was highly user-context aware and was able to answer questions even better.</li></ul><p>Being the sole developer of this project and creating everything from scratch, taught me a lot about the trade-offs between different Retrieval Strategies, LangGraph, RAGs and in general designing performant systems.</p></article><div class=post-tags><nav class=tags><ul class=flat><li><a href=/tags/rag>RAG</a></li><li><a href=/tags/ai>AI</a></li><li><a href=/tags/llms>LLMs</a></li><li><a href=/tags/langgraph>LangGraph</a></li><li><a href=/tags/whisper>Whisper</a></li><li><a href=/tags/text-to-speech>Text-To-Speech</a></li><li><a href=/tags/engineering>engineering</a></li></ul></nav></div><div><style>#share-buttons{display:inline-block;vertical-align:middle}#share-buttons:after{content:"";display:block;clear:both}#share-buttons>div{position:relative;text-align:left;height:36px;width:32px;float:left;text-align:center}#share-buttons>div>svg{height:16px;fill:#ebdbb2;margin-top:10px}#share-buttons>div:hover{cursor:pointer}#share-buttons>div.facebook:hover>svg{fill:#3b5998}#share-buttons>div.twitter:hover>svg{fill:#55acee}#share-buttons>div.linkedin:hover>svg{fill:#0077b5}#share-buttons>div.pinterest:hover>svg{fill:#cb2027}#share-buttons>div.gplus:hover>svg{fill:#dd4b39}#share-buttons>div.mail:hover>svg{fill:#7d7d7d}#share-buttons>div.instagram:hover>svg{fill:#c73b92}#share-buttons>div.facebook>svg{height:18px;margin-top:9px}#share-buttons>div.twitter>svg{height:20px;margin-top:8px}#share-buttons>div.linkedin>svg{height:19px;margin-top:7px}#share-buttons>div.pinterest>svg{height:20px;margin-top:9px}#share-buttons>div.gplus>svg{height:17px;margin-top:9px;position:relative;left:1px}#share-buttons>div.mail>svg{height:14px;margin-top:11px}</style><span style=color:#ebdbb2>Share on:</span><div id=share-buttons><div class=facebook title="Share this on Facebook" onclick='window.open("http://www.facebook.com/share.php?u=https://dm4p385.github.io/gbu-blog/posts/conversational-ai-assistant/")'><svg viewBox="0 0 1792 1792"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759H734V905H479V609h255V391q0-186 104-288.5T1115 0q147 0 228 12z"/></svg></div><div class=twitter title="Share this on Twitter" onclick='window.open("https://twitter.com/intent/tweet?url=https://dm4p385.github.io/gbu-blog/posts/conversational-ai-assistant/")'><svg viewBox="0 0 1792 1792"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5T1369.5 1125 1185 1335.5t-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5T285 1033q33 5 61 5 43 0 85-11-112-23-185.5-111.5T172 710v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5T884 653q-8-38-8-74 0-134 94.5-228.5T1199 256q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div><div class=linkedin title="Share this on Linkedin" onclick='window.open("https://www.linkedin.com/shareArticle?mini=true&url=https://dm4p385.github.io/gbu-blog/posts/conversational-ai-assistant/&title=&summary=&source=")'><svg viewBox="0 0 1792 1792"><path d="M477 625v991H147V625h330zm21-306q1 73-50.5 122T312 490h-2q-82 0-132-49t-50-122q0-74 51.5-122.5T314 148t133 48.5T498 319zm1166 729v568h-329v-530q0-105-40.5-164.5T1168 862q-63 0-105.5 34.5T999 982q-11 30-11 81v553H659q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5T1285 602q171 0 275 113.5t104 332.5z"/></svg></div><div class=mail title="Share this through Email" onclick='window.open("mailto:?&body=https://dm4p385.github.io/gbu-blog/posts/conversational-ai-assistant/")'><svg viewBox="0 0 1792 1792"><path d="M1792 710v794q0 66-47 113t-113 47H160q-66 0-113-47T0 1504V710q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 1e2-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38T639 1015q-91-64-262-182.5T172 690q-62-42-117-115.5T0 438q0-78 41.5-130T160 256h1472q65 0 112.5 47t47.5 113z"/></svg></div></div></div><style>.relatedcontainer{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:space-evenly;align-items:stretch;margin-top:0}.relateditem{display:block;color:var(--fg);max-width:30%;padding:2%;height:auto;margin:2%;box-shadow:0 .4rem .8rem rgba(0,0,0,.5);transition:.3s;border-radius:.5rem}.relateditem:hover{background-color:var(--bg1);color:var(--fg)}.relateditem p{text-overflow:ellipsis;margin:.5rem}.relateditem h6{font-size:1.2rem;margin:.5rem;color:var(--red)}.related{border-top:1px dashed var(--fg);margin-top:2rem}.related h5{font-size:1.6rem;margin-bottom:1rem;margin-top:1rem}@media only screen and (max-width:640px){.relateditem{min-width:60%}}@media only screen and (max-device-width:640px){.relateditem{min-width:60%}}</style><div class=related><h5>Related Content:</h5><div class=relatedcontainer><a class=relateditem href=/gbu-blog/posts/totp-based-qr/><div class=card-info><h6>Designing TOTP style QR Codes for Scale in Low Connectivity Environments</h6><p>By converting QR codes into time-synced tokens (like Google Auth), we cut server load, minimized retries, and made our fest app work flawlessly offline-first.</p></div></a></div></div></div><footer><nav><ul class=flat><li><div id=copyright>© 2025 Harsh Singh</div></li><li>[Built with <a href=https://gohugo.io>Hugo</a>]</li></ul></nav><script>feather.replace()</script></footer></div></body></html>