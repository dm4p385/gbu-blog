<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=icon type=image/svg href=https://thegoodbadundefined.blog/ico/favicon.svg><title>Event Streaming Analytics Platform - The good, the bad, and the [undefined]</title><meta name=description content="A blog about tech, engineering and all things in between."><meta name=keywords content="clicksteam,event-orchestration,realtime-analytics,PostgreSQL,Redis,FastAPI,kafka,engineering,"><link rel=preconnect href=https://fonts.gstatic.com><link href="https://fonts.googleapis.com/css2?family=Inconsolata&display=swap" rel=stylesheet><link rel=stylesheet type=text/css media=screen href=https://thegoodbadundefined.blog/css/main.css><link rel=stylesheet type=text/css media=screen href=https://thegoodbadundefined.blog/css/syntax.css><script src=https://unpkg.com/feather-icons></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Event Streaming Analytics Platform"><meta name=twitter:description content="Designing a User event streaming platform with horizontal scaling for customer engagement, real-time analytics and monitoring"><meta property="og:url" content="https://thegoodbadundefined.blog/posts/event-streaming-analytics-platform/"><meta property="og:site_name" content="The good, the bad, and the [undefined]"><meta property="og:title" content="Event Streaming Analytics Platform"><meta property="og:description" content="Designing a User event streaming platform with horizontal scaling for customer engagement, real-time analytics and monitoring"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2026-02-17T23:20:41+05:30"><meta property="article:modified_time" content="2026-02-17T23:20:41+05:30"><meta property="article:tag" content="Clicksteam"><meta property="article:tag" content="Event-Orchestration"><meta property="article:tag" content="Realtime-Analytics"><meta property="article:tag" content="PostgreSQL"><meta property="article:tag" content="Redis"><meta property="article:tag" content="FastAPI"><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("consent","default",{ad_storage:"denied",analytics_storage:"denied"})</script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","UA-XXXXXXXXX-X")</script><script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX-X"></script><script>for(var c,nameEQ=name+"=",ca=document.cookie.split(";"),i=0;i<ca.length;i++)c=ca[i],c.includes(nameEQ)&&gtag("consent","update",{ad_storage:"denied",analytics_storage:"granted"})</script></head><body><div class=container><header><div><h1 class=site-title><a href=https://thegoodbadundefined.blog/><span class=title-prefix>The good, the bad, and the </span><span class="bracket open">[</span><span class=undefined-inner>undefined</span><span class="bracket close">]</span></a></h1><div class=site-info><h2>A blog about tech, engineering and all things in between.</h2><nav class=social-nav><ul class=flat><li><a href=https://github.com/dm4p385 title=GitHub target=_blank><i data-feather=github></i></a></li><li><a href=https://x.com/harsh_singh58 title=X target=_blank><i data-feather=twitter></i></a></li><li><a href=https://www.linkedin.com/in/harshsingh58 title=LinkedIn target=_blank><i data-feather=linkedin></i></a></li><li><a rel=alternate type=application/rss+xml href=https://thegoodbadundefined.blog/index.xml title="The good, the bad, and the [undefined]"><i data-feather=rss></i></a></li></ul></nav></div><nav><ul class=flat><li><a href=/>Home</a></li><li><a href=/posts>Posts</a></li><li><a href=/tags>Tags</a></li><li><a href=/about>About</a></li></ul></nav></div></header><div class=content><div><h1 class=single-title>Event Streaming Analytics Platform</h1><p class=metadata>Published - 2026-02-17 | <i data-feather=clock style=width:12px;height:12px></i> 7min</p></div><article class=markdown><h2 id=background><strong>Background</strong></h2><p>Onboarding Journeys can be long, more so if they&rsquo;re for credit cards. The steps can be long drawn, they can lack clarity, and its very easy for users to get confused and dropoff entirely. Of course, this problem can be fixed by improving the UX, understanding what confuses users, which users are most likely to dropoff and which steps are most likely to cause these dropoffs. Once you figure why the users are dropping off maybe the next step is to claw some of them back with customer engagement tools? What if it was possible for analytics teams to get real time dropoff rates through different steps? What if it was possible for product owners to analyze results of their A/B testing in the journey?</p><p>These are some of the questions I asked myself while working on a funnel analysis of an onboarding journey. The answer to all of the above questions was an in-house event orchestration platform called Analytica (greek for science of analysis) that could power customer engagement tools, analytics, logging and basically anything that could require user triggered events.</p><h2 id=why-application-workflow-engines-just-aren><strong>Why application workflow engines just aren&rsquo;t enough</strong></h2><p>All the steps and transitions in our onboarding journey were being powered by an application workflow engine. Usually all of the application and transition data is stored on a RDBMS/NoSQL Database. However its not possible to directly use this as a source of truth for analytics. First of all, you would need to gather all of this data from different sources and dump them in a warehouse to power analytics tools which in itself is a non-trivial task.</p><p>Once you manage to figure what data to store in your warehouse now you would have to do this all over again if you ever wanted to analyze funnels for a different onboarding journey which uses a different workflow engine. Having faced these challenges already, I wanted to create a platform where I could emit events that are completely independent from the workflow engine logic. This meant I could store <strong>any</strong> user metric that I wanted to store in whichever manner/format I wanted. The frontend could also now directly send me a clickstream data of user events (page visits, click activity, etc) and all of it could now be tracked without having to rely on whims of the workflow engine.</p><h2 id=customer-engagement-aspect><strong>Customer Engagement aspect</strong></h2><p>As I had mentioned, a big part of clawing back dropped-off customers is to routinely send out engagement nudges based on internal logic. This meant using dedicated customer engagement tools that exist (e.g, Dittofeed) or developing your own for that matter. At the end of the day whichever route we decided to go, my idea for this platform was to be extensible above anything else. That meant while the event contracts between my platform and the event emitters were rigid, the engagement tool to be used for these events didnt have to be.</p><h2 id=real-time-analytics--alerting><strong>Real-time Analytics & Alerting</strong></h2><p>Since our platform relied on user events being emitted in real-time, this meant we could dump data into our warehouse in realtime which in turn meant we could connect BI Tools like Metabase or montoring tools like Grafana. This was a huge win specially since our existing methodology was to rely on time-lagged data sync jobs that would dump data to our warehouse. Suddenly it was possible to analyze dropoffs and raise alerts based on certain user activity thresholds in real-time.</p><h2 id=the-core-design-principles-and-tackling-scalability><strong>The core design principles, and tackling scalability</strong></h2><div style="margin:1rem 0;text-align:center"><img src=https://thegoodbadundefined.blog/images/p3_system_architecture.png alt="System Architecture Diagram" style=zgotmplz><figcaption style=font-size:.9rem;color:var(--muted,#666);margin-top:.4rem;text-align:center>High Level System architecture of our event streaming platform</figcaption></div><h3 id=extensibility-above-everything><strong>Extensibility above everything</strong></h3><p>Remember how I said my event contracts could be rigid? Well I lied, instead of relying on actual DTOs like API contracts, I would have validation schemas being fetched from the my platform&rsquo;s database. This meant 2 things</p><ol><li>My service acted as the single source of truth for all the event schemas, this meant I could tell what event captures what data.</li><li>I would not have to restart, recompile or redeploy my service even if an event schema changed, we could dynamically update the event schemas from the database as required.</li></ol><p>These event schema contracts will be be defined with use of <a href=https://json-schema.org>jsonschema</a> python library which allowed us to enforce required and type validations in the contract. Any events non-conforming to the defined schema stored in the database would simply be rejected in this validation layer.</p><h3 id=the-orchestrator-layer><strong>The Orchestrator Layer</strong></h3><p>Once the events had passed our validation layer, it was time for the orchestrator layer to send them our to the relevant consumers. What consumers you ask? Well the idea is simple, these consumers could be the downstream services (like warehouse, customer engagement and grafana) that are dependent on our user events. Since all consumers need not consume all events, this approach allowed us to configure only relevant events for specific consumers. This was achieved by introducing a transformation layer in between. Once validated, the orchestration layer would fetch relevant transformation configs from the db for this specific event type.</p><p>For example:</p><ol><li>For our warehouse consumer, we could define which fields are supposed to be mapped to what tables and what columns in the warehouse.</li><li>For our customer engagement tool, we could define which underlying API to call for which specific event.</li></ol><p>These transformation configs were stored as <a href=https://jinja.palletsprojects.com/en/stable/templates/>Jinja2 templates</a>. This allowed us to leverage all the templating functionality like auto-populating timstamps, generating UUIDs that Jinja offered without having to configure transformation logic for every event seperately. This meant our transformation were consistent and always in a format that our consumers expected.</p><h3 id=the-producer-consumer-pattern><strong>The Producer-Consumer Pattern</strong></h3><p>Staying true to our mantra of keeping everything extensible, we defined a consumer config table where all the consumer relevant configurations can be stored.</p><h3 id=addressing-the-elephant-in-the-room---scalability><strong>Addressing the elephant in the room - Scalability</strong></h3><p>As you could probably tell by now, the core problem my platform was about to face wasn&rsquo;t that of extensibility, it was that of scalability. Handling the sheer amount of throughput arriving from several event streams could easily overwhelm our system. To tackle this, I had 2 major optimizations in mind.</p><div style="margin:.5rem 0;text-align:center"><img src=/images/p3_elephant_in_the_room.png alt="Scalability meme" style=width:240px;max-width:28%;display:inline-block;border-radius:6px><figcaption style=font-size:.85rem;color:var(--muted,#666);margin-top:.35rem;text-align:center>Addressing the elephant in the room — scalability</figcaption></div><ol><li><p><strong>Cache Validation and Transformation Configs</strong>: Constantly fetching them from the database added enormous amounts of overhead and latency that can easily be skipped by caching these configs. Although this could have been faster and easier with a manual Config implementation that keeps things in-memory, I decided to go with a Redis based approach which I would explain shortly why. Upon calling the schema update API, we would simply fetch and invalidate this cached schema. This feature along with a TTL would prevent our cache values from becoming stale.</p></li><li><p><strong>Horizontally Scale EVERYTHING</strong>: If we simply de-coupled everything and relied purely on kafka for message queues, we could easily horizontally scale our system by simply adding more instances. This needed to happen for both, the central orchestration layer and the consumers. This meant all the consumers could be scaled completely independently. Since now we had multiple central orchestrator instances running, this meant we had to have a centralized cache store which is where Redis came into picture.</p></li></ol><p>Both of these optimizations completely de-coupled our platform and allowed us to scale individual components independently. We would rely on industry proven, battle tested tools like K8s for auto-scaling, Kafka for load balancing and queues and finally Redis for Cache management. This allowed our platform to focus purely on the business logic and not have to manage scalability itself.</p><h2 id=performance-benchmarks-and-final-thoughts>Performance Benchmarks and Final thoughts</h2><p>Using the entries being created by our application workflow engine, I arrived at an approximate event production rate of 48-50 eps (Events per second) at peak times. Since my platform captured way more events than this, I went ahead with a 2x approximation factor and set a benchmark of 100 eps.</p><p>With just 3 small instances of my central orchestration layer and consumers each, I was easily able to scale up to 300 eps with sub 20ms latency (p95 &lt;100ms). Since my approach was to rely on kafka, I was able to offload events from the central layer and store them in the consumer queues where my slower consumers could parse them at their own pace.</p><p>In future iterations, my plan is to implement batching in consumers for better throughput at the consumer end.</p><p>For now I was really satisfied with what I had achieved in terms of</p><ul><li>benchmarks (300 eps with built in resilliency and auto-scaling)</li><li>in terms of functionality (we successfuly demo&rsquo;d our customer engagement tool and warehouse analytics through this tool)</li><li>in short timespan (2 weeks)</li></ul><p>The challenge of balancing extensibility with scalability was a really fun experience overall and I was glad that I had the opportunity to design this systen from the ground up!</p></article><div class=post-tags><nav class=tags><ul class=flat><li><a href=/tags/clicksteam>clicksteam</a></li><li><a href=/tags/event-orchestration>event-orchestration</a></li><li><a href=/tags/realtime-analytics>realtime-analytics</a></li><li><a href=/tags/postgresql>PostgreSQL</a></li><li><a href=/tags/redis>Redis</a></li><li><a href=/tags/fastapi>FastAPI</a></li><li><a href=/tags/kafka>kafka</a></li><li><a href=/tags/engineering>engineering</a></li></ul></nav></div><div><style>#share-buttons{display:inline-block;vertical-align:middle}#share-buttons:after{content:"";display:block;clear:both}#share-buttons>div{position:relative;text-align:left;height:36px;width:32px;float:left;text-align:center}#share-buttons>div>svg{height:16px;fill:#ebdbb2;margin-top:10px}#share-buttons>div:hover{cursor:pointer}#share-buttons>div.facebook:hover>svg{fill:#3b5998}#share-buttons>div.twitter:hover>svg{fill:#55acee}#share-buttons>div.linkedin:hover>svg{fill:#0077b5}#share-buttons>div.pinterest:hover>svg{fill:#cb2027}#share-buttons>div.gplus:hover>svg{fill:#dd4b39}#share-buttons>div.mail:hover>svg{fill:#7d7d7d}#share-buttons>div.instagram:hover>svg{fill:#c73b92}#share-buttons>div.facebook>svg{height:18px;margin-top:9px}#share-buttons>div.twitter>svg{height:20px;margin-top:8px}#share-buttons>div.linkedin>svg{height:19px;margin-top:7px}#share-buttons>div.pinterest>svg{height:20px;margin-top:9px}#share-buttons>div.gplus>svg{height:17px;margin-top:9px;position:relative;left:1px}#share-buttons>div.mail>svg{height:14px;margin-top:11px}</style><span style=color:#ebdbb2>Share on:</span><div id=share-buttons><div class=facebook title="Share this on Facebook" onclick='window.open("http://www.facebook.com/share.php?u=https://thegoodbadundefined.blog/posts/event-streaming-analytics-platform/")'><svg viewBox="0 0 1792 1792"><path d="M1343 12v264h-157q-86 0-116 36t-30 108v189h293l-39 296h-254v759H734V905H479V609h255V391q0-186 104-288.5T1115 0q147 0 228 12z"/></svg></div><div class=twitter title="Share this on Twitter" onclick='window.open("https://twitter.com/intent/tweet?url=https://thegoodbadundefined.blog/posts/event-streaming-analytics-platform/")'><svg viewBox="0 0 1792 1792"><path d="M1684 408q-67 98-162 167 1 14 1 42 0 130-38 259.5T1369.5 1125 1185 1335.5t-258 146-323 54.5q-271 0-496-145 35 4 78 4 225 0 401-138-105-2-188-64.5T285 1033q33 5 61 5 43 0 85-11-112-23-185.5-111.5T172 710v-4q68 38 146 41-66-44-105-115t-39-154q0-88 44-163 121 149 294.5 238.5T884 653q-8-38-8-74 0-134 94.5-228.5T1199 256q140 0 236 102 109-21 205-78-37 115-142 178 93-10 186-50z"/></svg></div><div class=linkedin title="Share this on Linkedin" onclick='window.open("https://www.linkedin.com/shareArticle?mini=true&url=https://thegoodbadundefined.blog/posts/event-streaming-analytics-platform/&title=&summary=&source=")'><svg viewBox="0 0 1792 1792"><path d="M477 625v991H147V625h330zm21-306q1 73-50.5 122T312 490h-2q-82 0-132-49t-50-122q0-74 51.5-122.5T314 148t133 48.5T498 319zm1166 729v568h-329v-530q0-105-40.5-164.5T1168 862q-63 0-105.5 34.5T999 982q-11 30-11 81v553H659q2-399 2-647t-1-296l-1-48h329v144h-2q20-32 41-56t56.5-52 87-43.5T1285 602q171 0 275 113.5t104 332.5z"/></svg></div><div class=mail title="Share this through Email" onclick='window.open("mailto:?&body=https://thegoodbadundefined.blog/posts/event-streaming-analytics-platform/")'><svg viewBox="0 0 1792 1792"><path d="M1792 710v794q0 66-47 113t-113 47H160q-66 0-113-47T0 1504V710q44 49 101 87 362 246 497 345 57 42 92.5 65.5t94.5 48 110 24.5h2q51 0 110-24.5t94.5-48 92.5-65.5q170-123 498-345 57-39 1e2-87zm0-294q0 79-49 151t-122 123q-376 261-468 325-10 7-42.5 30.5t-54 38-52 32.5-57.5 27-50 9h-2q-23 0-50-9t-57.5-27-52-32.5-54-38T639 1015q-91-64-262-182.5T172 690q-62-42-117-115.5T0 438q0-78 41.5-130T160 256h1472q65 0 112.5 47t47.5 113z"/></svg></div></div></div><style>.relatedcontainer{display:flex;flex-direction:row;flex-wrap:wrap;justify-content:space-evenly;align-items:stretch;margin-top:0}.relateditem{display:block;color:var(--fg);max-width:30%;padding:2%;height:auto;margin:2%;box-shadow:0 .4rem .8rem rgba(0,0,0,.5);transition:.3s;border-radius:.5rem}.relateditem:hover{background-color:var(--bg1);color:var(--fg)}.relateditem p{text-overflow:ellipsis;margin:.5rem}.relateditem h6{font-size:1.2rem;margin:.5rem;color:var(--red)}.related{border-top:1px dashed var(--fg);margin-top:2rem}.related h5{font-size:1.6rem;margin-bottom:1rem;margin-top:1rem}@media only screen and (max-width:640px){.relateditem{min-width:60%}}@media only screen and (max-device-width:640px){.relateditem{min-width:60%}}</style><div class=related><h5>Related Content:</h5><div class=relatedcontainer><a class=relateditem href=/posts/conversational-ai-assistant/><div class=card-info><h6>Building a conversational Voice & Text AI Assistant</h6><p>How we made a human-like voice and text AI assistant to help reduce dropoffs in our onboarding journey.</p></div></a><a class=relateditem href=/posts/totp-based-qr/><div class=card-info><h6>Designing TOTP style QR Codes for Scale in Low Connectivity Environments</h6><p>By converting QR codes into time-synced tokens (like Google Auth), we cut server load, minimized retries, and made our fest app work flawlessly offline-first.</p></div></a></div></div></div><footer><nav><ul class=flat><li><div id=copyright>© 2026 Harsh Singh</div></li><li>[Built with <a href=https://gohugo.io>Hugo</a>]</li></ul></nav><script>feather.replace()</script></footer></div></body></html>